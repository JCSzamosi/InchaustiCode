Inchausti Stats Book
====================

## Chapter 3

```{r, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
packages.needed<-c("ggplot2","reshape2","gridExtra","mvtnorm","bbmle")
lapply(packages.needed,FUN=require,character.only=T)
theme_set(theme_bw())
```

### Maximum Likelihood Numerical Estimation

`dbinom()` gives the density (i.e. the probability) of the data given the 
parameters.

```{r}
dbinom(x = 6, size = 10, prob = 0.5)
```

We typically look at likelihoods on the log scale:

```{r}
dbinom(x = 6, size = 10, prob = 0.5, log = TRUE)
```

Try this for a range of probabilities ranging from 0.001 to 0.999 and put it all
in a data frame:

```{r}
DF4 = data.frame(prob = seq(from = 0.001, to = 0.999, by = 0.001))
```

Create a new version of the `dbinom` function, for some reason

```{r}
LBinom = function(prob, x, size, log = TRUE){
  return(dbinom(x = x, size = size, prob = prob, log = log))
}
```

Calculate the probability of the observed data for all the values of pi given in
`DF4`

```{r}
DF4$Lik = LBinom(prob = DF4$prob, x = 6, size = 10)
head(DF4)
```

Plot the log-likelihoods

```{r}
ll_plt = ggplot(DF4, aes(prob, Lik)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1, 0.1))
ll_plt
```

Identify the position of the highest likelihood value

```{r}
which.max(DF4$Lik)
```

It is the 600th value. Let's pull that row out of the data frame

```{r}
DF4[which.max(DF4$Lik),]
```

So out of the 999 probability values we tried, a probability of 0.6 is the one
that is most likely to produce our observed data.

What if the exact right answer doesn't happen to be in the set we tried? How can 
we reliably estimate it?

We don't have to do that ourselves. The package `bbmle` has a function, `mle2()`
to do that for us. Fortunately, I added the `bbmle` package to the set of 
packages we need for this chapter above.

First we need a function that gives us the _negative_ log likelihood. It will be 
almost identical to our `LBinom()` function, but multiplies the result by -1.

```{r}
negL = function(prob, x, size, log = TRUE){
  -1 * dbinom(x = x, size = size, prob = prob, log = log)
}
```

That is the function that `mle2()` wants in order to estimate the ML value:

```{r}
prob.estimated = mle2(minuslogl = negL, data = list(x = 6, size = 10),
                      start = list(prob = 0.2))
summary(prob.estimated)
```

This gives us the estimate for pi (0.6) as well as its standard error, the 
z-value test statistic, and its associated p-value in a test of difference from 
0.

The `confint()` function gives us the confidence interval (95%, two-tailed by 
default).

```{r}
confint(prob.estimated)
```

Let's re-make figure 3.3 from the book. Inchausti skips a lot of steps in 
talking about this, so let's see if I can bring them back in:
