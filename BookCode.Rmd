Inchausti Stats Book
====================

## Chapter 3

```{r, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
packages.needed<-c("ggplot2","reshape2","gridExtra","mvtnorm","bbmle")
lapply(packages.needed,FUN=require,character.only=T)
theme_set(theme_bw())
```

### Maximum Likelihood Numerical Estimation

`dbinom()` gives the density (i.e. the probability) of the data given the 
parameters.

```{r}
dbinom(x = 6, size = 10, prob = 0.5)
```

We typically look at likelihoods on the log scale:

```{r}
dbinom(x = 6, size = 10, prob = 0.5, log = TRUE)
```

Try this for a range of probabilities ranging from 0.001 to 0.999 and put it all
in a data frame:

```{r}
DF4 = data.frame(prob = seq(from = 0.001, to = 0.999, by = 0.001))
```

Create a new version of the `dbinom` function, for some reason

```{r}
LBinom = function(prob, x, size, log = TRUE){
  return(dbinom(x = x, size = size, prob = prob, log = log))
}
```

Calculate the probability of the observed data for all the values of pi given in
`DF4`

```{r}
DF4$Lik = LBinom(prob = DF4$prob, x = 6, size = 10)
head(DF4)
```

Plot the log-likelihoods

```{r}
ll_plt = ggplot(DF4, aes(prob, Lik)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1, 0.1))
ll_plt
```

Identify the position of the highest likelihood value

```{r}
which.max(DF4$Lik)
```

It is the 600th value. Let's pull that row out of the data frame

```{r}
DF4[which.max(DF4$Lik),]
```

So out of the 999 probability values we tried, a probability of 0.6 is the one
that is most likely to produce our observed data.

What if the exact right answer doesn't happen to be in the set we tried? How can 
we reliably estimate it?

We don't have to do that ourselves. The package `bbmle` has a function, `mle2()`
to do that for us. Fortunately, I added the `bbmle` package to the set of 
packages we need for this chapter above.

First we need a function that gives us the _negative_ log likelihood. It will be 
almost identical to our `LBinom()` function, but multiplies the result by -1.

```{r}
negL = function(prob, x, size, log = TRUE){
  -1 * dbinom(x = x, size = size, prob = prob, log = log)
}
```

That is the function that `mle2()` wants in order to estimate the ML value:

```{r}
prob.estimated = mle2(minuslogl = negL, data = list(x = 6, size = 10),
                      start = list(prob = 0.2))
summary(prob.estimated)
```

This gives us the estimate for pi (0.6) as well as its standard error, the 
z-value test statistic, and its associated p-value in a test of difference from 
0.

The `confint()` function gives us the confidence interval (95%, two-tailed by 
default).

```{r}
confint(prob.estimated)
```

Let's re-make figure 3.3 from the book. Inchausti skips a lot of steps in 
talking about this, so let's see if I can bring them back in:

Step 1 is to take the negative log likelihood. As I explain in my [likelihood 
page](./Likelihood.html), this is accomplished by multiplying the likelihood by
-1:

```{r}
DF4$negLik = -1 * DF4$Lik
```

If we plot that, we get the same plot, but inverted:

```{r}
nll_plt = ggplot(DF4, aes(prob, negLik)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1, 0.1))
nll_plt
```

The next step is to convert this to what is called a "likelihood profile". This
is accomplished by normalizing to the minimum value (i.e. subtracting the 
minimum value from everything) and then taking the square root. The reasons for 
this are complicated, but have to do with making the confidence interval work
correctly.

Here is a function to do that:

```{r}
lik_prof = function(x){
  p = x - min(x)
  p = sqrt(p)
  return(p)
}
```

Let's add it to the data frame and plot it:

```{r}
DF4$profLik = lik_prof(DF4$negLik)
pll_plt = ggplot(DF4, aes(prob, profLik)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, 1, 0.1))
pll_plt
```

And add the lines indicating the confidence intervals

```{r}
ci = confint(prob.estimated)
# Get the bottom of the confidence interval
low_ci = min(ci)
# Get the top of the confidence interval
hi_ci = max(ci)

# Get the negative log likelihood at the bottom of the CI and normalize to the
# minimum likelihood
low_lik = negL(low_ci, 6, 10) - min(DF4$negLik)
# Get the negative log likelihood at the top of the CI and normalize to the 
# minimum likelihood
hi_lik = negL(hi_ci, 6, 10) - min(DF4$negLik)

# Add the CI to the plot, taking the square root of the negative log likelihood
pll_plt = pll_plt +
  geom_segment(x = low_ci, xend = low_ci, y = 0, yend = sqrt(low_lik),
               linetype = 2) +
  geom_segment(x = hi_ci, xend = hi_ci, y = 0, yend = sqrt(hi_lik),
               linetype = 2) +
  geom_segment(x = low_ci, xend = hi_ci, y = sqrt(low_lik), yend = sqrt(hi_lik),
               linetype = 2)
pll_plt
```

Ichausti appears to be limiting the plot to the range 0.2 - 0.9 on the x-axis.

I don't know why he's doing that, but let's do it here:

```{r}
pll_plt + 
  ylim(0, 2.5) + 
  scale_x_continuous(breaks = seq(0.2, 0.9, 0.1), limits = c(0.2, 0.9))
```

This plot has the same shape as Inchausti's, but the y-axis is on a different 
scale. I will be honest, I have no idea why. I would but even odds on whether he
or I made a mistake.
